{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d680f230-0c97-3375-f47a-16cb2cb12e77"
   },
   "source": [
    "**The idea is:**\n",
    "\n",
    " - Feature reduction with PCA\n",
    " - Data transformation (log, hot encoding, nan)\n",
    " - Test different regression models\n",
    "\n",
    "**Things found:**\n",
    "\n",
    "- Applying log transformation really increases the accuracy.\n",
    "- Using PCA with 36 components makes the learning and testing much (much much) faster.\n",
    "- Removing columns with more than 1000 NaNs gives better result than applying \"mean\" to them.\n",
    "- There are outliers. Instead of removing them, using Huber seems to provide a good result. Huber is a model robust to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c2d0b1a9-a6e3-7ace-ab73-6375fccb210a"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import seaborn\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", \"./input\"]).decode(\"utf8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "acc6da78-9491-216e-56c1-61d2f3e6da81"
   },
   "source": [
    "## Data Load ##\n",
    "\n",
    "I mix data and test to manipulate all the data just once. SalePrice is extracted to its own variable \"labels\". Finally, SalesPrice is remove from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6fad5e5f-097d-0916-8549-8ada7d29cdfe"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/train.csv')\n",
    "labels=train[\"SalePrice\"]\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "data = pd.concat([train,test],ignore_index=True)\n",
    "data = data.drop(\"SalePrice\", 1)\n",
    "ids = test[\"Id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e769e045-f25a-a25d-047a-a03b931e43a4"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35a5b9cb-3383-ba7b-efb5-4662bdd46c47"
   },
   "outputs": [],
   "source": [
    "# Count the number of rows in train\n",
    "train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "041d9472-1f4a-3b15-d624-cfb9ea59710b"
   },
   "outputs": [],
   "source": [
    "# Count the number of rows in total\n",
    "data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "eb27ffcc-3f7b-d0e1-3887-e955307c21b0"
   },
   "outputs": [],
   "source": [
    "# Count the number of NaNs each column has.\n",
    "nans=pd.isnull(data).sum()\n",
    "nans[nans>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "92ba1dbf-0b3b-7d19-ba9f-702c59991612"
   },
   "outputs": [],
   "source": [
    "# Remove id and columns with more than a thousand missing values\n",
    "data=data.drop(\"Id\", 1)\n",
    "data=data.drop(\"Alley\", 1)\n",
    "data=data.drop(\"Fence\", 1)\n",
    "data=data.drop(\"MiscFeature\", 1)\n",
    "data=data.drop(\"PoolQC\", 1)\n",
    "data=data.drop(\"FireplaceQu\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1978e020-6ca4-124e-1063-3c8262fbc3e7"
   },
   "outputs": [],
   "source": [
    "# Count the column types\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "82816c00-ff5e-247e-2a55-04f96e2ed313"
   },
   "source": [
    "## Data Manipulation ##\n",
    "\n",
    "- Apply hot encoding, convert categorical variable into dummy/indicator variables.\n",
    "- Fill NaN with median for that column.\n",
    "- Log transformation.\n",
    "- Change -inf to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "24356f20-6c36-de3e-38b1-1a3f4e01f867"
   },
   "outputs": [],
   "source": [
    "all_columns = data.columns.values\n",
    "non_categorical = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \n",
    "                   \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"1stFlrSF\", \n",
    "                   \"2ndFlrSF\", \"LowQualFinSF\", \"GrLivArea\", \"GarageArea\", \n",
    "                   \"WoodDeckSF\", \"OpenPorchSF\", \"EnclosedPorch\", \"3SsnPorch\", \n",
    "                   \"ScreenPorch\",\"PoolArea\", \"MiscVal\"]\n",
    "\n",
    "categorical = [value for value in all_columns if value not in non_categorical]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "bd7155af-7e2e-12a0-5ef3-030bfc16a945"
   },
   "outputs": [],
   "source": [
    "#Â One Hot Encoding and nan transformation\n",
    "data = pd.get_dummies(data)\n",
    "\n",
    "imp = Imputer(missing_values='NaN', strategy='most_frequent', axis=0)\n",
    "data = imp.fit_transform(data)\n",
    "\n",
    "# Log transformation\n",
    "data = np.log(data)\n",
    "labels = np.log(labels)\n",
    "\n",
    "# Change -inf to 0 again\n",
    "data[data==-np.inf]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4affb7d5-578e-6862-243f-630f21a05934"
   },
   "source": [
    "## Feature reduction ##\n",
    "\n",
    "There are many features, so I am going to use PCA to reduce them. The idea is to start with n_components = number of columns. Then select the number of components that add up to 1 variance_ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6954036d-1415-4bcf-d67b-ac7a410f8a65"
   },
   "outputs": [],
   "source": [
    "pca = PCA(whiten=True)\n",
    "pca.fit(data)\n",
    "variance = pd.DataFrame(pca.explained_variance_ratio_)\n",
    "np.cumsum(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "e9a015a1-642e-0a50-2219-2e2f92db582a"
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=36,whiten=True)\n",
    "pca = pca.fit(data)\n",
    "dataPCA = pca.transform(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8901b708-1465-3e16-aca4-f3ea1bac97d8"
   },
   "source": [
    "## Data Model Selection ##\n",
    "\n",
    "Simple test to run multiple models against our data. First, with raw features. No PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a2d1aafa-ea0a-5753-8886-581e88fe1cc0"
   },
   "outputs": [],
   "source": [
    "# Split traing and test\n",
    "train = data[:1460]\n",
    "test = data[1460:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a85a2fa0-59ec-c055-f088-ab0a29ea68bd"
   },
   "outputs": [],
   "source": [
    "# R2 Score\n",
    "\n",
    "def lets_try(train,labels):\n",
    "    results={}\n",
    "    def test_model(clf):\n",
    "        \n",
    "        cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "        r2 = make_scorer(r2_score)\n",
    "        r2_val_score = cross_val_score(clf, train, labels, cv=cv,scoring=r2)\n",
    "        scores=[r2_val_score.mean()]\n",
    "        return scores\n",
    "\n",
    "    clf = linear_model.LinearRegression()\n",
    "    results[\"Linear\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Ridge()\n",
    "    results[\"Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.BayesianRidge()\n",
    "    results[\"Bayesian Ridge\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.HuberRegressor()\n",
    "    results[\"Hubber\"]=test_model(clf)\n",
    "    \n",
    "    clf = linear_model.Lasso(alpha=1e-4)\n",
    "    results[\"Lasso\"]=test_model(clf)\n",
    "    \n",
    "    clf = BaggingRegressor()\n",
    "    results[\"Bagging\"]=test_model(clf)\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    results[\"RandomForest\"]=test_model(clf)\n",
    "    \n",
    "    clf = AdaBoostRegressor()\n",
    "    results[\"AdaBoost\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR()\n",
    "    results[\"SVM RBF\"]=test_model(clf)\n",
    "    \n",
    "    clf = svm.SVR(kernel=\"linear\")\n",
    "    results[\"SVM Linear\"]=test_model(clf)\n",
    "    \n",
    "    results = pd.DataFrame.from_dict(results,orient='index')\n",
    "    results.columns=[\"R Square Score\"] \n",
    "    results=results.sort(columns=[\"R Square Score\"],ascending=False)\n",
    "    results.plot(kind=\"bar\",title=\"Model Scores\")\n",
    "    axes = plt.gca()\n",
    "    axes.set_ylim([0.5,1])\n",
    "    return results\n",
    "\n",
    "lets_try(train,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f8f5cc3a-09bf-629b-c8de-0c3ef68e226b"
   },
   "source": [
    "Now, let's try the same but using data with PCA applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "af6df7bc-a49f-a5d8-7dc1-391aa0b9b2e3"
   },
   "outputs": [],
   "source": [
    "# Split traing and test\n",
    "train = dataPCA[:1460]\n",
    "test = dataPCA[1460:]\n",
    "\n",
    "lets_try(train,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "77b2423f-1b69-0d57-8cc7-ebf3b6d54b23"
   },
   "outputs": [],
   "source": [
    "cv = KFold(n_splits=5,shuffle=True,random_state=45)\n",
    "\n",
    "parameters = {'alpha': [1000,100,10],\n",
    "              'epsilon' : [1.2,1.25,1.50],\n",
    "              'tol' : [1e-10]}\n",
    "\n",
    "clf = linear_model.HuberRegressor()\n",
    "r2 = make_scorer(r2_score)\n",
    "grid_obj = GridSearchCV(clf, parameters, cv=cv,scoring=r2)\n",
    "grid_fit = grid_obj.fit(train, labels)\n",
    "best_clf = grid_fit.best_estimator_ \n",
    "\n",
    "best_clf.fit(train,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8f0e5865-ea95-a696-9d3a-31e0b67315cc"
   },
   "source": [
    "Simple Neural Network\n",
    "---------------------\n",
    "\n",
    "Now I am going to try a simple neural network, to see if i can improve the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "89da5919-66d6-22f7-65cc-f3f8bd3b5c80"
   },
   "outputs": [],
   "source": [
    "# Shape the labels\n",
    "labels_nl = labels\n",
    "labels_nl = labels_nl.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "c1c49f31-68e9-5364-6c8c-eb1b94109230"
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "r2 = tflearn.R2()\n",
    "net = tflearn.input_data(shape=[None, train.shape[1]])\n",
    "net = tflearn.fully_connected(net, 30, activation='linear')\n",
    "net = tflearn.fully_connected(net, 10, activation='linear')\n",
    "net = tflearn.fully_connected(net, 1, activation='linear')\n",
    "sgd = tflearn.SGD(learning_rate=0.1, lr_decay=0.01, decay_step=100)\n",
    "net = tflearn.regression(net, optimizer=sgd,loss='mean_square',metric=r2)\n",
    "model = tflearn.DNN(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "be5ae562-8e8d-c204-f786-6cb0a065b1d1"
   },
   "outputs": [],
   "source": [
    "model.fit(train, labels_nl,show_metric=True,validation_set=0.2,shuffle=True,n_epoch=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3c100e90-e9a9-c028-22a7-4c74ee31ab0c"
   },
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "\n",
    "predictions_huber = best_clf.predict(test)\n",
    "predictions_DNN = model.predict(test)\n",
    "predictions_huber = np.exp(predictions_huber)\n",
    "predictions_DNN = np.exp(predictions_DNN)\n",
    "predictions_DNN = predictions_DNN.reshape(-1,)\n",
    "\n",
    "sub = pd.DataFrame({\n",
    "        \"Id\": ids,\n",
    "        \"SalePrice\": predictions_DNN\n",
    "    })\n",
    "\n",
    "sub.to_csv(\"prices_submission.csv\", index=False)\n",
    "#print(sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b3730440-457e-7629-9eec-2faaef1608eb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
